import os
import json
from pathlib import Path
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)
from transformers import PreTrainedTokenizerFast
import logging
from tqdm import tqdm
log_dir = "../logs"
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    handlers=[
        logging.StreamHandler(), 
        logging.FileHandler(os.path.join(log_dir, "training.log"))  
    ]
)

def get_training_corpus():
    for file_path in tqdm(jsonl_files, desc="processing files", unit="file"):
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                data = json.loads(line)
                if "text" in data:
                    yield data["text"]


corpus_dir = Path("./corpus/skypile_2020_head")
jsonl_files = list(corpus_dir.glob("**/*.jsonl"))
if not jsonl_files:
    raise ValueError("No training files found")


special_tokens = [
    "<|endoftext|>",
    "<|im_start|>",
    "<|im_end|>",
]


tokenizer = Tokenizer(models.BPE())


tokenizer.normalizer = normalizers.Sequence([
    normalizers.NFKC(),
    normalizers.StripAccents(),
])


tokenizer.pre_tokenizer = pre_tokenizers.Sequence(
    [
        pre_tokenizers.Split(r"[\r\n]", "isolated"),   
        pre_tokenizers.Split(r"\s?[A-Za-zÂµÃ€-Ã–Ã˜-Ã¶Ã¸-ÆºÆ¼-Æ¿Ç„-Ê“Ê•-Ê¯Í°-Í³Í¶Í·Í»-Í½Í¿Î†Îˆ-ÎŠÎŒÎ-Î¡Î£-ÏµÏ·-ÒÒŠ-Ô¯Ô±-Õ–á‚ -áƒ…á -áµá¸-á½á²-á²ºá²½-á²¿á´€-á´«áµ«-áµ·áµ¹-á¶šá¸€-á¼•á¼˜-á¼á¼ -á½…á½ˆ-á½á½-á½—á½™á½›á½á½Ÿ-á½½á¾€-á¾´á¾¶-á¾¼á¾¾á¿‚-á¿„á¿†-á¿Œá¿-á¿“á¿–-á¿›á¿ -á¿¬á¿²-á¿´á¿¶-á¿¼â„‚â„‡â„Š-â„“â„•â„™-â„â„¤â„¦â„¨â„ª-â„­â„¯-â„´â„¹â„¼-â„¿â……-â…‰â…â†ƒâ†„â°€-â±»â±¾-â³¤â³«-â³®â³²â³³ê™€-ê™­êš€-êš›êœ¢-ê¯ê±-ê‡ê‹-êê­°-ê®¿ï¬€-ï¬†ï¬“-ï¬—ï¼¡-ï¼ºï½-ï½šğ€-ğ‘ğ’°-ğ““ğ“˜-ğ“»ğ²€-ğ²²ğ³€-ğ³²ğ‘¢ -ğ‘£Ÿğ¤€-ğ¥ƒ]+","isolated"), 
        pre_tokenizers.Split(r"\s?[!-/:-~ï¼-ï¼ï¼š-ï½â€˜-â€Ÿã€€-ã€‚]+", "isolated"),   
        pre_tokenizers.Split(r"\s+$", "isolated"), 
        pre_tokenizers.Split("[ä¸€-é¾¥à €-ä¸€ê°€-íŸ¿]+","isolated"),
        pre_tokenizers.Digits(individual_digits=True),
        pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False),
    ])

# tokenizer.post_processor = None
tokenizer.post_processor = processors.ByteLevel(use_regex=True, trim_offsets=False, add_prefix_space=True)

tokenizer.decoder = decoders.ByteLevel(
    use_regex=True,
    trim_offsets=True,
    add_prefix_space=False  
)


trainer = trainers.BpeTrainer(
    vocab_size=12800,
    special_tokens=special_tokens, 
    show_progress=True  
)

logging.info("Training tokenizer...")
tokenizer.train_from_iterator(
    get_training_corpus(),
    trainer=trainer,
    length=sum(1 for _ in get_training_corpus())  
)

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token=None,  
    eos_token="<|endoftext|>",
    pad_token="<|endoftext|>",
    unk_token=None,  
    additional_special_tokens=["<|im_start|>", "<|im_end|>"],
    model_max_length=32768,  
    clean_up_tokenization_spaces=False,  
    errors="replace", 
    split_special_tokens=False,  
)
logging.info("train done ")
output_dir = Path("./llm_tokenizer")
output_dir.mkdir(exist_ok=True, parents=True)

wrapped_tokenizer.save_pretrained(output_dir)
logging.info(f"Tokenizer saved to {output_dir}")

QWEN2_CHAT_TEMPLATE = (
    "{% for message in messages %}"
    "{% if loop.first and messages[0]['role'] != 'system' %}"
    "{{ '<|im_start|>system\nYou are a helpful assistant<|im_end|>\n' }}"
    "{% endif %}"
    "{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}"
    "{% endfor %}"
    "{% if add_generation_prompt %}"
    "{{ '<|im_start|>assistant\n' }}"
    "{% endif %}"
)

wrapped_tokenizer.chat_template = QWEN2_CHAT_TEMPLATE

wrapped_tokenizer.save_pretrained(output_dir)

messages = [
    {"role": "user", "content": "ä½ å¥½ï¼"},
    {"role": "assistant", "content": "ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ"},
    {"role": "user", "content": "Pythonæ€ä¹ˆç”¨ï¼Ÿ"}
]

formatted = wrapped_tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
logging.info("\næ ¼å¼åŒ–åçš„å¯¹è¯:")
logging.info(formatted)

test_text = "æ·±åº¦å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚Deep learning is a subset of machine learning."
logging.info("\nTest tokenization:")
encoded = wrapped_tokenizer.tokenize(test_text)
logging.info(encoded)
logging.info(encoded)
decoded_text = wrapped_tokenizer.decode(encoded) 
logging.info(decoded_text)
logging.info(decoded_text)